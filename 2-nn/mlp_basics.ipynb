{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks\n",
    "\n",
    "After going through Lesson 2, you should now have a much better intuition about neural networks. PyTorch and other deep learning frameworks will let you easily manipulate these structures. In this exercise, you will dive into the challenges of computation and data structures for versatile systems such as Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:21.486946Z",
     "start_time": "2018-12-09T18:43:21.480892Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Object-oriented Programming\n",
    "\n",
    "Contrary to previous exercise on notebooks, on top of being able to define functions, you can define classes (your pytorch models are instances of a class).\n",
    "\n",
    "In Python, a basic class (let's call it MyClass) is defined as below:\n",
    "```\n",
    "class MyClass:\n",
    "\n",
    "    def __init__(self, obj_name):\n",
    "        self.name = obj_name\n",
    "\n",
    "```\n",
    "\n",
    "Here we defined the class MyClass and its constructor __init__ (more or so the function that creates the object). This way, we defined the Class as having a single attribute called name.\n",
    "```\n",
    "my_obj = MyClass('my_obj_name')\n",
    "```\n",
    "will thus create a class instance with the attribute name = 'my_obj_name'. The only mandatory argument of the __init__ method is self, it's a way for Python to relate to the instance you will create.\n",
    "\n",
    "\n",
    "Last thing you need to know is that you can define functions specific to your class that are called methods. You can define as many as you want with whichever arguments you see fit.\n",
    "```\n",
    "class MyClass:\n",
    "\n",
    "    def __init__(self, obj_name):\n",
    "        self.name = obj_name\n",
    "    \n",
    "    def my_method():\n",
    "        print('Hello World!')\n",
    "    \n",
    "    def whatsyourname(self):\n",
    "        return self.name\n",
    "\n",
    "```\n",
    "\n",
    "Above we defined two methods: one that prints a hard-coded famous string, and the other than return your attribute. Where a function f, is used with f(input) in Python, a method of a given obj is used with my_obj.my_method(input) in Python.\n",
    "```\n",
    "my_obj = MyClass('my_obj_name')\n",
    "my_obj.my_method()\n",
    "name = my_obj.whatsyourname()\n",
    "print(name)\n",
    "```\n",
    "will return\n",
    "```\n",
    "Hello World!\n",
    "my_obj_name\n",
    "```\n",
    "\n",
    "Notice that everytime we added self as an argument for class methods, we didn't have to specify it when calling this method. Check this resource https://realpython.com/python3-object-oriented-programming/ for more information.\n",
    "\n",
    "Alright, you're ready now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "Your naive feed-forward perceptron model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: input features of size N\n",
    "- Outputs: scalar activation value (size 1)\n",
    "- Parameters: weights, bias, activation\n",
    "\n",
    "Given X the input features' vector of size N, W the weights, b the bias, and activation the activation function of the perceptron, you aim at reproducing the operation we will call forward, as follows:\n",
    "$$ \\forall (X_i)_{i\\ \\in\\ [1, N]}\\ \\in\\ \\mathbb{R}^N,\\ forward((X_i)_{i\\ \\in\\ [1, N]}) = activation\\Bigg(\\Big(\\sum_{i=1}^{N} W_i * X_i\\Big) + b\\Bigg) $$\n",
    "which we will rewrite more concisely using the convention that for two 1-dimensional vectors of identical size A and B, AB represents the dot-product of both vectors.\n",
    "$$ \\forall X\\ \\in\\ \\mathbb{R}^N, forward(X) = activation(WX + b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.841636Z",
     "start_time": "2018-12-09T17:16:32.838299Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will generate your inputs for this sub-section\n",
    "def get_inputs(input_size):\n",
    "    \n",
    "    inputs = np.random.rand(input_size)\n",
    "    # Let's take our weights and bias in [-1, 1] rather than [0, 1]\n",
    "    weights = 2 * np.random.rand(input_size) - 1\n",
    "    bias = 2 * np.random.rand(1) - 1\n",
    "    \n",
    "    return inputs, weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:32:30.485318Z",
     "start_time": "2018-12-09T17:32:30.481008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) (8,) (1,)\n",
      "[0.52475643 0.43194502 0.29122914 0.61185289 0.13949386 0.29214465\n",
      " 0.36636184 0.45606998]\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 8\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, bias = get_inputs(input_size)\n",
    "print(inputs.shape, weights.shape, bias.shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.850846Z",
     "start_time": "2018-12-09T17:16:32.848114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check if everything is alright by computing WX + b\n",
    "lin_comb = 0\n",
    "# Check if this is indeed 1-dimensional\n",
    "print(lin_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation (non-linearity)\n",
    "Now edit the NaivePerceptron class of the naive_perceptron.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:26.744497Z",
     "start_time": "2018-12-09T18:43:26.737957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define an activation function (feel free to change this)\n",
    "def sigmoid(x): return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.864806Z",
     "start_time": "2018-12-09T17:16:32.855839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34733063])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the NaivePerceptron methods __init__ and forward to compute the correct activation\n",
    "from naive_perceptron import NaivePerceptron\n",
    "p = NaivePerceptron(weights, bias, activation=sigmoid)\n",
    "p.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You managed to build your own feed-forward perceptron in base Python!\n",
    "Last thing to do: edit the __author__ and __maintainer__ at the top of your naive_perceptron file to put yours, you're in charge now ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first layer\n",
    "Alright, we got 1 perceptron but state-of-the-art architectures have thousands of them. So we need to be able to stack many of them in a consistent way: a perceptron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:29:06.842983Z",
     "start_time": "2018-12-09T18:29:06.839901Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will generate your inputs for this sub-section\n",
    "def get_inputs(input_size, output_size):\n",
    "    \n",
    "    inputs = np.random.rand(input_size)\n",
    "    # Let's take our weights and bias in [-1, 1] rather than [0, 1]\n",
    "    weights = 2 * np.random.rand(output_size, input_size) - 1\n",
    "    bias = 2 * np.random.rand(output_size) - 1\n",
    "    \n",
    "    return inputs, weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a layer of M perceptrons, with the same input X as previously, and Wj and bj being the weigths and the bias of the j-th perceptron in the layer, we want to define a new forward operation that will give us the output features of the layer:\n",
    "$$ \\forall X\\ \\in\\ \\mathbb{R}^N, forward(X) = (activation(W_jX + b_j))_{j\\ \\in\\ [1, M]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:34:28.924274Z",
     "start_time": "2018-12-09T17:34:28.918166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,) (8, 64) (8,)\n",
      "[0.96563203 0.80839735 0.30461377 0.09767211 0.68423303]\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 64\n",
    "nb_perceptrons = 8\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, biases = get_inputs(input_size, nb_perceptrons)\n",
    "print(inputs.shape, weights.shape, biases.shape)\n",
    "print(inputs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the NaiveLayer class of the naive_layer.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:41:26.591207Z",
     "start_time": "2018-12-09T17:41:26.585680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16338088e-01, 1.25305140e-01, 9.95996845e-01, 2.00959868e-03,\n",
       "       8.67587232e-01, 7.51686419e-01, 9.96706154e-01, 5.39813449e-04])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the NaiveLayer methods __init__ and forward to compute the correct activation\n",
    "from naive_layer import NaiveLayer\n",
    "l = NaiveLayer(weights, biases, activation=sigmoid)\n",
    "l.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that we could have defined a different activation function for each unit. But first, for parallel computing it is not a good idea, and second, this would give less consistency in a given layer output feature map.\n",
    "Don't forget to edit the author and maintainer's name at the top of your naive_layer.py file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP comeback\n",
    "Since we have a common activation function for the layer, why not computing it all at once?\n",
    "We have naively defined separate perceptrons that computed individually their activation values.\n",
    "\n",
    "![MLP](http://pubs.sciepub.com/ajmm/3/3/1/bigimage/fig5.png \"Multi-layer perceptron\")\n",
    "Source: Science and Education Publishing\n",
    "\n",
    "As per the representation above, we can see a parametrization by units in a layer, or by their connections with input/output layers. As we are able to forward information, it would be good to prepare the weight and bias update. In our previous definition, the arguments weights, biases and activation were enough to define the entire layer.\n",
    "\n",
    "So let's try to simplify and speed up our layer modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:20:58.141429Z",
     "start_time": "2018-12-09T18:20:58.133080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 64\n",
    "nb_perceptrons = 8\n",
    "inputs = np.random.rand(64)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time will let the layer initialize its weights and biases itself. We just want to make sure its initializing the right amount of it.\n",
    "Edit the layer class of the layer.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:21:10.337242Z",
     "start_time": "2018-12-09T18:21:10.315303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32327157, 0.18556123, 0.94712284, 0.13010543, 0.01774532,\n",
       "       0.98100981, 0.82891384, 0.51758245])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the Layer methods __init__ and forward to compute the correct activation\n",
    "from layer import Layer\n",
    "l = Layer(input_size, nb_perceptrons, activation=sigmoid)\n",
    "l.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we are computing the result as fast as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:29:10.994731Z",
     "start_time": "2018-12-09T18:29:10.978253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,) (64, 1024) (64,)\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 1024\n",
    "nb_perceptrons = 64\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, biases = get_inputs(input_size, nb_perceptrons)\n",
    "print(inputs.shape, weights.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:31:17.607831Z",
     "start_time": "2018-12-09T18:31:17.600860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024-unit naive layer\n",
      "Duration: 0:00:00.000124\n",
      "\n",
      "1024-unit layer\n",
      "Duration: 0:00:00.000130\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "l = NaiveLayer(weights, biases, activation=sigmoid)\n",
    "stime = datetime.now()\n",
    "print(f\"{input_size}-unit naive layer\")\n",
    "print(f\"Duration: {datetime.now() - stime}\")\n",
    "\n",
    "l = Layer(input_size, nb_perceptrons, activation=sigmoid)\n",
    "stime = datetime.now()\n",
    "print(f\"\\n{input_size}-unit layer\")\n",
    "print(f\"Duration: {datetime.now() - stime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now time to create a proper MLP with some depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:51.466484Z",
     "start_time": "2018-12-09T18:43:51.463628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the number of inputs (feel free to change this)\n",
    "input_size = 1024\n",
    "inputs = np.random.rand(input_size)\n",
    "# We'll define the number of units in each consecutive layer (feel free to change this)\n",
    "layer_output_sizes = [256, 64, 64, 32, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:52.508159Z",
     "start_time": "2018-12-09T18:43:52.482211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97895182, 0.2379899 , 0.87799318, 0.07823132, 0.90545582,\n",
       "       0.8116613 , 0.01358233, 0.20985839])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the MLP methods __init__ and forward to compute the correct network output\n",
    "from layer import MLP\n",
    "mlp = MLP(input_size, layer_output_sizes, activation=sigmoid)\n",
    "mlp.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is really good, do you see how close you are from having your own framework building blocks?\n",
    "This missing part is about the update of this network's parameters, with the famously known backpropagation.\n",
    "\n",
    "But you're done for the day, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
