{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks\n",
    "\n",
    "After going through Lesson 2, you should now have a much better intuition about neural networks. PyTorch and other deep learning frameworks will let you easily manipulate these structures. In this exercise, you will dive into the challenges of computation and data structures for versatile systems such as Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:21.486946Z",
     "start_time": "2018-12-09T18:43:21.480892Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Object-oriented Programming\n",
    "\n",
    "Contrary to previous exercise on notebooks, on top of being able to define functions, you can define classes (your pytorch models are instances of a class).\n",
    "\n",
    "![Class](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/CPT-OOP-objects_and_classes_-_attmeth.svg/300px-CPT-OOP-objects_and_classes_-_attmeth.svg.png \"Basic car class\")\n",
    "Source: wikibooks.org\n",
    "\n",
    "### Attributes and constructor\n",
    "In Python, a basic class (let's call it MyClass) is defined as below:\n",
    "```\n",
    "class MyClass:\n",
    "\n",
    "    def __init__(self, obj_name):\n",
    "        self.name = obj_name\n",
    "\n",
    "```\n",
    "\n",
    "Here we defined the class MyClass and its constructor __init__ (more or so the function that creates the object). This way, we defined the Class as having a single attribute called name.\n",
    "```\n",
    "my_obj = MyClass('my_obj_name')\n",
    "```\n",
    "will thus create a class instance with the attribute name = 'my_obj_name'. The only mandatory argument of the __init__ method is self, it's a way for Python to relate to the instance you will create.\n",
    "\n",
    "\n",
    "### Methods\n",
    "Last thing you need to know is that you can define functions specific to your class that are called methods. You can define as many as you want with whichever arguments you see fit.\n",
    "```\n",
    "class MyClass:\n",
    "\n",
    "    def __init__(self, obj_name):\n",
    "        self.name = obj_name\n",
    "    \n",
    "    def my_method():\n",
    "        print('Hello World!')\n",
    "    \n",
    "    def whatsyourname(self):\n",
    "        return self.name\n",
    "\n",
    "```\n",
    "\n",
    "Above we defined two methods: one that prints a hard-coded famous string, and the other than return your attribute. Where a function f, is used with f(input) in Python, a method of a given obj is used with my_obj.my_method(input) in Python.\n",
    "```\n",
    "my_obj = MyClass('my_obj_name')\n",
    "my_obj.my_method()\n",
    "name = my_obj.whatsyourname()\n",
    "print(name)\n",
    "```\n",
    "will return\n",
    "```\n",
    "Hello World!\n",
    "my_obj_name\n",
    "```\n",
    "\n",
    "Notice that everytime we added self as an argument for class methods, we didn't have to specify it when calling this method.\n",
    "\n",
    "### Example\n",
    "We model a basic Phone usage/charging interactions as follows:\n",
    "- the phone has an owner, and physical characteristics. The owner is specific to the object, but the physical characteristics are the same for all objects of this class.\n",
    "- we want to be able to check the owner of the phone, charge it, and use it.\n",
    "\n",
    "\n",
    "```\n",
    "class Phone:\n",
    "\n",
    "    charge_speed = 0.1\n",
    "    max_charge = 1.\n",
    "    draining_speed = 0.02\n",
    "\n",
    "    def __init__(self, owner):\n",
    "        self.owner = owner\n",
    "        # Set initial charge to 0\n",
    "        self.battery = 0.\n",
    "    \n",
    "    def charge(self, charge_duration):\n",
    "        self.battery += charge_duration * self.charge_speed\n",
    "        self.battery = max(self.max_charge, self.battery)\n",
    "    \n",
    "    def whosyourowner(self):\n",
    "        return self.owner\n",
    "    \n",
    "    def use(self, usage_duration):\n",
    "        if self.battery == 0.:\n",
    "            raise ValueError(\"No more battery\")\n",
    "        else:\n",
    "            self.battery -= usage_duration * self.draining_speed\n",
    "            self.battery = min(0., self.battery)\n",
    "\n",
    "```\n",
    "So now, this will throw you an error since your phone isn't initially charged:\n",
    "```\n",
    "my_phone = Phone('John')\n",
    "my_phone.whosyourowner()\n",
    "my_phone.use(1)\n",
    "```\n",
    "but this will work:\n",
    "```\n",
    "my_phone = Phone('John')\n",
    "my_phone.charge(1)\n",
    "my_phone.use(1.5)\n",
    "```\n",
    "\n",
    "Check this resource https://realpython.com/python3-object-oriented-programming/ for more information.\n",
    "Alright, you're ready now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "Your naive feed-forward perceptron model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: input features of size N\n",
    "- Outputs: scalar activation value (size 1)\n",
    "- Parameters: weights, bias, activation\n",
    "\n",
    "Given X the input features' vector of size N, W the weights, b the bias, and activation the activation function of the perceptron, you aim at reproducing the operation we will call forward, $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$, as follows:\n",
    "$$ \\forall (X_i)_{i\\ \\in\\ [1, N]}\\ \\in\\ \\mathbb{R}^N,\\ forward((X_i)_{i\\ \\in\\ [1, N]}) = activation\\Bigg(\\Big(\\sum_{i=1}^{N} W_i * X_i\\Big) + b\\Bigg) $$\n",
    "\n",
    "![Perceptron](https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png \"Perceptron representation\")\n",
    "*Source: Towards Data Science*\n",
    "\n",
    "which we will rewrite more concisely using the convention that for two 1-dimensional vectors of identical size A and B, AB represents the dot-product of both vectors.\n",
    "$$ \\forall X\\ \\in\\ \\mathbb{R}^N, forward(X) = activation(WX + b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.841636Z",
     "start_time": "2018-12-09T17:16:32.838299Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will generate your inputs for this sub-section\n",
    "def get_inputs(input_size):\n",
    "    \n",
    "    inputs = np.random.rand(input_size)\n",
    "    # Let's take our weights and bias in [-1, 1] rather than [0, 1]\n",
    "    weights = 2 * np.random.rand(input_size) - 1\n",
    "    bias = 2 * np.random.rand(1) - 1\n",
    "    \n",
    "    return inputs, weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operation\n",
    "Let's focus on the weighted sum and the bias as they represent the linear part of the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:32:30.485318Z",
     "start_time": "2018-12-09T17:32:30.481008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) (8,) (1,)\n",
      "[0.52475643 0.43194502 0.29122914 0.61185289 0.13949386 0.29214465\n",
      " 0.36636184 0.45606998]\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 8\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, bias = get_inputs(input_size)\n",
    "print(inputs.shape, weights.shape, bias.shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking $X = (X_i)_{i\\ \\in\\ [1, N]}$ as inputs, and $W = (W_i)_{i\\ \\in\\ [1, N]}$ as weights, with $b$ being the bias, compute the value of $lin\\_comb: \\mathbb{R}^N \\rightarrow \\mathbb{R}$:\n",
    "$$ lin\\_comb(X, W, b) = \\Big(\\sum_{i=1}^{N} W_i * X_i\\Big) + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.850846Z",
     "start_time": "2018-12-09T17:16:32.848114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check if everything is alright by computing WX + b\n",
    "lin_comb = 0\n",
    "# Check if this is indeed 1-dimensional\n",
    "print(lin_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation (non-linearity)\n",
    "If you remember the lesson material, we need to introduce a non-linearity in our perceptron model. This operation is called the activation. \n",
    "\n",
    "For feed-forward purposes, you can take any real function operating on scalar inputs $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Being able to then compare the output of the activation function is good, but using the entire real space is a bit of an overkill. So it comes handy to squeeze the output space to a narrower segment.\n",
    "\n",
    "A common activation function is the sigmoid (also called logistic function) $\\sigma$ which meets the squeezing requirement $\\sigma: \\mathbb{R} \\rightarrow [0, 1]$\n",
    "\n",
    "$$ \\forall x\\ \\in\\ \\mathbb{R},\\ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png \"Sigmoid plot\")\n",
    "Source: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:26.744497Z",
     "start_time": "2018-12-09T18:43:26.737957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define an activation function (feel free to change this)\n",
    "def sigmoid(x): return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like earlier we discussed defining a class, its attributes and methods, we will implement a naive perceptron class. Its attributes would be its weights and bias, and apart from the constructor, we will need a forward method.\n",
    "\n",
    "Now edit the NaivePerceptron class of the naive_perceptron.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:16:32.864806Z",
     "start_time": "2018-12-09T17:16:32.855839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34733063])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the NaivePerceptron methods __init__ and forward to compute the correct activation\n",
    "from naive_perceptron import NaivePerceptron\n",
    "p = NaivePerceptron(weights, bias, activation=sigmoid)\n",
    "p.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You managed to build your own feed-forward perceptron in base Python!\n",
    "Last thing to do: edit the __author__ and __maintainer__ at the top of your naive_perceptron.py file to put yours, you're in charge now ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first layer\n",
    "Alright, we got 1 perceptron but state-of-the-art architectures have thousands of them. So we need to be able to stack many of them in a consistent way: a perceptron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:29:06.842983Z",
     "start_time": "2018-12-09T18:29:06.839901Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will generate your inputs for this sub-section\n",
    "def get_inputs(input_size, output_size):\n",
    "    \n",
    "    inputs = np.random.rand(input_size)\n",
    "    # Let's take our weights and bias in [-1, 1] rather than [0, 1]\n",
    "    weights = 2 * np.random.rand(output_size, input_size) - 1\n",
    "    bias = 2 * np.random.rand(output_size) - 1\n",
    "    \n",
    "    return inputs, weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a layer of M perceptrons, with the same input X as previously, and Wj and bj being the weigths and the bias of the j-th perceptron in the layer, we want to define a new forward operation, $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ , that will give us the output features of the layer:\n",
    "$$ \\forall X\\ \\in\\ \\mathbb{R}^N, forward(X) = (activation(W_jX + b_j))_{j\\ \\in\\ [1, M]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:34:28.924274Z",
     "start_time": "2018-12-09T17:34:28.918166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,) (8, 64) (8,)\n",
      "[0.96563203 0.80839735 0.30461377 0.09767211 0.68423303]\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 64\n",
    "nb_perceptrons = 8\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, biases = get_inputs(input_size, nb_perceptrons)\n",
    "print(inputs.shape, weights.shape, biases.shape)\n",
    "print(inputs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you already implemented in your NaivePerceptron class, we will define a NaiveLayer that is composed of multiple NaivePerceptron objects.\n",
    "\n",
    "Edit the NaiveLayer class of the naive_layer.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T17:41:26.591207Z",
     "start_time": "2018-12-09T17:41:26.585680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16338088e-01, 1.25305140e-01, 9.95996845e-01, 2.00959868e-03,\n",
       "       8.67587232e-01, 7.51686419e-01, 9.96706154e-01, 5.39813449e-04])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the NaiveLayer methods __init__ and forward to compute the correct activation\n",
    "from naive_layer import NaiveLayer\n",
    "l = NaiveLayer(weights, biases, activation=sigmoid)\n",
    "l.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that we could have defined a different activation function for each unit. But first, for parallel computing it is not a good idea, and second, this would give less consistency in a given layer output feature map.\n",
    "Don't forget to edit the author and maintainer's name at the top of your naive_layer.py file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP comeback\n",
    "Since we have a common activation function for the layer, why not computing it all at once?\n",
    "We have naively defined separate perceptrons that computed individually their activation values.\n",
    "\n",
    "![MLP](http://pubs.sciepub.com/ajmm/3/3/1/bigimage/fig5.png \"Multi-layer perceptron\")\n",
    "*Source: Science and Education Publishing*\n",
    "\n",
    "As per the representation above, we can see a parametrization by units in a layer, or by their connections with input/output layers. As we are able to forward information, it would be good to prepare the weight and bias update. In our previous definition, the arguments weights, biases and activation were enough to define the entire layer.\n",
    "\n",
    "So let's try to simplify and speed up our layer modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:20:58.141429Z",
     "start_time": "2018-12-09T18:20:58.133080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 64\n",
    "nb_perceptrons = 8\n",
    "inputs = np.random.rand(64)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering a single layer, of M perceptrons with N inputs, the activation value $f_j$ of the j-th perceptron is: \n",
    "\n",
    "$$ \\forall X\\ \\in \\mathbb{R}^N, \\forall j\\ \\in [1,M], f_j = activation\\Bigg(\\Big(\\sum_{i=1}^{N}W_{j,i}X_i\\Big) + b_j\\Bigg) = (activation(W_jX + b_j))_{j\\ \\in\\ [1, M]}$$\n",
    "where $W_{j,i}$ represents the weight of the i-th input $X_i$ for the j-th perceptron.\n",
    "\n",
    "Let's enlarge our definition of the activation function that will operate on a larger space than scalars, $activation: \\mathbb{R}^k \\rightarrow \\mathbb{R}^k$ (vectorized function programmingly speaking).\n",
    "\n",
    "We can now have a matrix understanding of the previous expression, with $f\\ \\in \\mathbb{R}^M$ the vector of all perceptrons activation values,\n",
    "\n",
    "$$ \\forall X\\ \\in \\mathbb{R}^N, f(X) = activation\\Big(\\Big(\\Big(\\sum_{i=1}^{N}W_{j,i}X_i\\Big) + b_j\\Big)_{j\\ \\in\\ [1, M]}\\Big) = activation\\Bigg(\\begin{bmatrix}\n",
    "    (\\sum_{i=1}^{N}W_{1,i}X_i) + b_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    (\\sum_{i=1}^{N}W_{M,i}X_i) + b_M \n",
    "\\end{bmatrix}\\Bigg)$$\n",
    "\n",
    "$$ = activation\\Bigg(\\begin{bmatrix}\n",
    "    (\\sum_{i=1}^{N}W_{1,i}X_i) \\\\\n",
    "    \\vdots \\\\\n",
    "    (\\sum_{i=1}^{N}W_{M,i}X_i)\n",
    "\\end{bmatrix} + \\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_M\\end{bmatrix}\\Bigg)\n",
    "= activation\\Bigg(\\begin{bmatrix}\n",
    "    W_{1,1} & \\cdots & W_{1,N} \\\\\n",
    "    \\vdots & & \\vdots \\\\\n",
    "    W_{M,1} & \\cdots & W_{M,N}\n",
    "\\end{bmatrix}*\\begin{bmatrix}X_1 \\\\ \\vdots \\\\ X_N\\end{bmatrix} + \\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_M\\end{bmatrix}\\Bigg)$$\n",
    "\n",
    "Thus with  $ \\mathbb{M}_{a,b}(\\mathbb{R})$ being the ensemble of real matrices of size a * b,\n",
    "$$ \\forall\\ W\\ \\in\\ \\mathbb{M}_{M,N}(\\mathbb{R}), \\forall\\ b\\ \\in\\ \\mathbb{R}^M, \\forall\\ X\\ \\in\\ \\mathbb{R}^N,\\ f = activation(WX + b) $$\n",
    "\n",
    "As you can see, we can condense all layer operations in a single matrix operation, plus the cross-unit activation. Let's implement this!\n",
    "\n",
    "This time will let the layer initialize its weights and biases itself. We just want to make sure its initializing the right amount of it.\n",
    "Edit the layer class of the layer.py file.\n",
    "Use the refresher about Object-Oriented Programming (OOP) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:21:10.337242Z",
     "start_time": "2018-12-09T18:21:10.315303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32327157, 0.18556123, 0.94712284, 0.13010543, 0.01774532,\n",
       "       0.98100981, 0.82891384, 0.51758245])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the Layer methods __init__ and forward to compute the correct activation\n",
    "from layer import Layer\n",
    "l = Layer(input_size, nb_perceptrons, activation=sigmoid)\n",
    "l.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we are computing the result as fast as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:29:10.994731Z",
     "start_time": "2018-12-09T18:29:10.978253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,) (64, 1024) (64,)\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of inputs\n",
    "input_size = 1024\n",
    "nb_perceptrons = 64\n",
    "\n",
    "# Load the inputs\n",
    "inputs, weights, biases = get_inputs(input_size, nb_perceptrons)\n",
    "print(inputs.shape, weights.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:31:17.607831Z",
     "start_time": "2018-12-09T18:31:17.600860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024-unit naive layer\n",
      "Duration: 0:00:00.000124\n",
      "\n",
      "1024-unit layer\n",
      "Duration: 0:00:00.000130\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "l = NaiveLayer(weights, biases, activation=sigmoid)\n",
    "stime = datetime.now()\n",
    "print(f\"{input_size}-unit naive layer\")\n",
    "print(f\"Duration: {datetime.now() - stime}\")\n",
    "\n",
    "l = Layer(input_size, nb_perceptrons, activation=sigmoid)\n",
    "stime = datetime.now()\n",
    "print(f\"\\n{input_size}-unit layer\")\n",
    "print(f\"Duration: {datetime.now() - stime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now time to create a proper MLP with some depth.\n",
    "\n",
    "![Deep MLP](http://cs231n.github.io/assets/nn1/neural_net2.jpeg \"Deep MLP\")\n",
    "*Source: Stanford CS231n*\n",
    "\n",
    "For L consecutive layers with individual forward function $(f_l)_{l\\ \\in\\ [1, L]}$, where the input is of size N and the last layer output is of size O, the MLP forwarding operation $F: \\mathbb{R}^N \\rightarrow \\mathbb{R}^O$ will be:\n",
    "\n",
    "$$\\forall\\ X\\ \\in\\ \\mathbb{R}^N, F(X) = (f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1)(X) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:51.466484Z",
     "start_time": "2018-12-09T18:43:51.463628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the number of inputs (feel free to change this)\n",
    "input_size = 1024\n",
    "inputs = np.random.rand(input_size)\n",
    "# We'll define the number of units in each consecutive layer (feel free to change this)\n",
    "layer_output_sizes = [256, 64, 64, 32, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now implement your MLP by forwarding the input through each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T18:43:52.508159Z",
     "start_time": "2018-12-09T18:43:52.482211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97895182, 0.2379899 , 0.87799318, 0.07823132, 0.90545582,\n",
       "       0.8116613 , 0.01358233, 0.20985839])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: edit the MLP methods __init__ and forward to compute the correct network output\n",
    "from layer import MLP\n",
    "mlp = MLP(input_size, layer_output_sizes, activation=sigmoid)\n",
    "mlp.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is really good, do you see how close you are from having your own framework building blocks?\n",
    "This missing part is about the update of this network's parameters, with the famously known backpropagation.\n",
    "\n",
    "But you're done for the day, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
